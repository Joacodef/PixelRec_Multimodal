# scripts/create_splits.py
#!/usr/bin/env python
"""Create standardized splits for evaluation, with an option to sample the dataset."""
import sys
from pathlib import Path
import pandas as pd
import json
import hashlib
from typing import List, Optional # Added Optional

# Add parent directory to system path to allow importing local modules
sys.path.append(str(Path(__file__).resolve().parent.parent))

from src.config import Config
from src.data.splitting import DataSplitter


def filter_by_activity(interactions_df: pd.DataFrame, min_user_interactions: int, min_item_interactions: int) -> pd.DataFrame:
    """
    Filters the interactions DataFrame to ensure users and items have a minimum number of interactions.

    This function first filters items, then users, to ensure that only active entities
    are included in the final dataset.

    Args:
        interactions_df: The input DataFrame containing 'user_id' and 'item_id' columns.
        min_user_interactions: Minimum number of interactions required for a user to be included.
        min_item_interactions: Minimum number of interactions required for an item to be included.

    Returns:
        A filtered DataFrame containing only interactions from active users and items.
    """
    # Filter by item interactions
    if min_item_interactions > 0:
        item_counts = interactions_df['item_id'].value_counts()
        active_items = item_counts[item_counts >= min_item_interactions].index
        interactions_df = interactions_df[interactions_df['item_id'].isin(active_items)]
        print(f"Filtered by item activity (min {min_item_interactions}): {len(interactions_df)} interactions, {interactions_df['item_id'].nunique()} items remain.")

    # Filter by user interactions
    if min_user_interactions > 0:
        user_counts = interactions_df['user_id'].value_counts()
        active_users = user_counts[user_counts >= min_user_interactions].index
        interactions_df = interactions_df[interactions_df['user_id'].isin(active_users)]
        print(f"Filtered by user activity (min {min_user_interactions}): {len(interactions_df)} interactions, {interactions_df['user_id'].nunique()} users remain.")
    return interactions_df


def create_splits(config_path: str, sample_n: Optional[int] = None): # Added sample_n argument
    """
    Creates standardized train/validation/test splits for fair evaluation of recommender systems.

    The split ratios, minimum interactions per user/item, and random state are
    read from the provided configuration file. The splitting strategy defaults
    to stratified. Optionally samples the dataset before splitting.

    Args:
        config_path: Path to the YAML configuration file.
        sample_n: Optional number of interactions to sample from the dataset before splitting.
    """
    
    # Load configuration from the specified YAML file.
    config = Config.from_yaml(config_path)
    # Access the data splitting configuration directly.
    splitting_config = config.data.splitting
    
    # Load processed interactions data. This data is expected to be
    # generated by a prior preprocessing step.
    interactions_df = pd.read_csv(config.data.processed_interactions_path)
    original_total_interactions = len(interactions_df) # Store original size for metadata

    # --- New Sampling Logic ---
    if sample_n is not None:
        if sample_n > 0 and sample_n <= len(interactions_df):
            print(f"Sampling {sample_n} interactions from the dataset (original size: {len(interactions_df)})...")
            interactions_df = interactions_df.sample(n=sample_n, random_state=splitting_config.random_state).reset_index(drop=True)
            print(f"Dataset size after sampling: {len(interactions_df)} interactions.")
        elif sample_n > len(interactions_df):
            print(f"Warning: Requested sample_n ({sample_n}) is larger than the dataset size ({len(interactions_df)}). Using the full dataset.")
        else:
            print("Warning: Invalid sample_n value. Using the full dataset.")
    # --- End of New Sampling Logic ---
    
    # Ensure the output directory for splits exists.
    output_path = Path(config.data.split_data_path)
    output_path.mkdir(parents=True, exist_ok=True)

    # Filter interactions based on minimum activity criteria as specified in the config.
    # This ensures that only users and items with sufficient historical interactions
    # are included in the dataset for splitting.
    interactions_df = filter_by_activity(
        interactions_df,
        min_user_interactions=splitting_config.min_interactions_per_user,
        min_item_interactions=splitting_config.min_interactions_per_item
    )

    if interactions_df.empty:
        print("Error: No interactions remaining after activity filtering (and potential sampling). Cannot create splits.")
        sys.exit(1)
    
    # Initialize the data splitter with the configured random state for reproducibility.
    splitter = DataSplitter(random_state=splitting_config.random_state)
    
    # Calculate the total ratio for the training and validation sets.
    # This is used in the first step of a two-step stratified split.
    total_train_val_ratio = splitting_config.train_final_ratio + splitting_config.val_final_ratio
    
    # Perform basic validation for the configured ratios.
    if total_train_val_ratio <= 0:
        raise ValueError("Sum of train_final_ratio and val_final_ratio must be greater than zero.")
    if splitting_config.test_final_ratio < 0 or splitting_config.test_final_ratio >= 1:
        raise ValueError("test_final_ratio must be between 0 and 1 (exclusive of 1).")
    
    # Check if ratios sum to 1.0 and normalize them if there are floating-point inaccuracies.
    sum_of_all_ratios = splitting_config.train_final_ratio + splitting_config.val_final_ratio + splitting_config.test_final_ratio
    if abs(sum_of_all_ratios - 1.0) > 1e-6:
        print(f"Warning: Configured train_final_ratio ({splitting_config.train_final_ratio}), "
              f"val_final_ratio ({splitting_config.val_final_ratio}), and "
              f"test_final_ratio ({splitting_config.test_final_ratio}) do not sum to 1.0 (sum is {sum_of_all_ratios}). Normalizing...")
        if sum_of_all_ratios > 0:
            splitting_config.train_final_ratio /= sum_of_all_ratios
            splitting_config.val_final_ratio /= sum_of_all_ratios
            splitting_config.test_final_ratio /= sum_of_all_ratios
            total_train_val_ratio = splitting_config.train_final_ratio + splitting_config.val_final_ratio
        else: # Handle case where all ratios are zero, defaulting to a 60/20/20 split.
            print("All configured split ratios are zero. Defaulting to 60/20/20 split.")
            splitting_config.train_final_ratio = 0.6
            splitting_config.val_final_ratio = 0.2
            splitting_config.test_final_ratio = 0.2
            total_train_val_ratio = 0.8

    # First split: Separate out the test set from the remaining data (train_val_df).
    # A stratified split is used to ensure each user is represented in both parts,
    # as much as possible, for consistent evaluation.
    train_val_df, test_df = splitter.stratified_split(
        interactions_df, 
        train_ratio=total_train_val_ratio,
        min_interactions_per_user=splitting_config.min_interactions_per_user # Ensure min_interactions_per_user is passed here
    )
    
    # Second split: Divide the `train_val_df` into training and validation sets.
    # The ratio for this split is adjusted to reflect the final desired proportions
    # within the original dataset.
    train_ratio_for_second_split = (splitting_config.train_final_ratio / total_train_val_ratio) if total_train_val_ratio > 0 else 0.0
    
    train_df, val_df = splitter.stratified_split(
        train_val_df,
        train_ratio=train_ratio_for_second_split,
        min_interactions_per_user=splitting_config.min_interactions_per_user # And here
    )
    
    # Save the generated data splits to CSV files.
    train_df.to_csv(output_path / "train.csv", index=False)
    val_df.to_csv(output_path / "val.csv", index=False)
    test_df.to_csv(output_path / "test.csv", index=False)
    
    # Create and save metadata about the generated splits.
    # This includes creation date, configuration used, dataset sizes,
    # and unique user/item counts in each split.
    metadata = {
        "creation_date": pd.Timestamp.now().isoformat(),
        "config_file": config_path,
        "original_total_interactions_before_sampling": original_total_interactions, # New metadata field
        "requested_sample_n": sample_n if sample_n is not None else "Not Sampled", # New metadata field
        "interactions_after_sampling_before_filtering": len(interactions_df) if sample_n is not None else original_total_interactions, # New metadata field
        "total_interactions_after_activity_filtering": len(interactions_df), # Updated meaning: after sampling (if any) AND activity filtering
        "train_size": len(train_df),
        "val_size": len(val_df),
        "test_size": len(test_df),
        "train_users": train_df['user_id'].nunique(),
        "val_users": val_df['user_id'].nunique(),
        "test_users": test_df['user_id'].nunique(),
        "train_items": train_df['item_id'].nunique(),
        "val_items": val_df['item_id'].nunique(),
        "test_items": test_df['item_id'].nunique(),
        "data_hash_after_sampling_and_filtering": hashlib.md5(interactions_df.to_csv().encode()).hexdigest(), # Hash of the data that was actually split
        "split_ratios_configured": {
            "train_final_ratio": splitting_config.train_final_ratio,
            "val_final_ratio": splitting_config.val_final_ratio,
            "test_final_ratio": splitting_config.test_final_ratio
        },
        "random_state_used": splitting_config.random_state,
        "min_interactions_per_user_used": splitting_config.min_interactions_per_user,
        "min_interactions_per_item_used": splitting_config.min_interactions_per_item
    }
    
    with open(output_path / "split_metadata.json", 'w') as f:
        json.dump(metadata, f, indent=2)
    
    print(f"Created evaluation splits in {output_path}")
    print(json.dumps(metadata, indent=2))

    # Validate for data leakage if specified in the configuration.
    if splitting_config.validate_no_leakage:
        print("\nPerforming leakage validation...")
        # Check overlap between training and test sets.
        train_test_stats = splitter.get_split_statistics(train_df, test_df)
        print(f"Train-Test Overlap: Users - {train_test_stats['user_overlap']} ({train_test_stats['user_overlap_ratio']:.2%}), Items - {train_test_stats['item_overlap']} ({train_test_stats['item_overlap_ratio']:.2%})")

        # Check overlap between training and validation sets.
        train_val_stats = splitter.get_split_statistics(train_df, val_df)
        print(f"Train-Validation Overlap: Users - {train_val_stats['user_overlap']} ({train_val_stats['user_overlap_ratio']:.2%}), Items - {train_val_stats['item_overlap']} ({train_val_stats['item_overlap_ratio']:.2%})")
        
        # Check overlap between validation and test sets.
        val_test_stats = splitter.get_split_statistics(val_df, test_df)
        print(f"Validation-Test Overlap: Users - {val_test_stats['user_overlap']} ({val_test_stats['user_overlap_ratio']:.2%}), Items - {val_test_stats['item_overlap']} ({val_test_stats['item_overlap_ratio']:.2%})")

        # For stratified splits, user/item overlap is expected and generally desired
        # for warm-start evaluation. The check here primarily confirms no *unexpected*
        # leakage (e.g., specific interactions being duplicated).
        if train_test_stats['user_overlap_ratio'] < 1.0 or train_test_stats['item_overlap_ratio'] < 1.0:
             # This condition might need adjustment based on the *expected* overlap for stratified splits.
             # For a true stratified split, you expect high overlap for users if min_interactions_per_user is met for splits.
            print("Info: Some users/items in test set might not be present in the training set, or vice-versa. This is typical for stratified splits, especially if filtering removes users/items entirely from one potential set after the initial split attempts by the splitter logic.")
        
        print("Leakage validation completed. For stratified splits, user/item overlap between sets is expected as interactions from the same user/item are distributed across splits.")

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="Create standardized splits for evaluation")
    parser.add_argument('--config', type=str, default='configs/simple_config.yaml',
                        help='Path to configuration file (e.g., configs/simple_config.yaml)')

    parser.add_argument('--sample_n', type=int, default=None, # Changed from sample_size to sample_n
                        help='Optional: Number of random interactions to sample from the dataset before splitting. If not provided, uses the full dataset.')

    args = parser.parse_args()
    
    create_splits(args.config, args.sample_n)