# scripts/create_splits.py
#!/usr/bin/env python
"""Create standardized splits for evaluation"""
import sys
from pathlib import Path
import pandas as pd
import json
import hashlib
from typing import List

# Add parent directory to system path to allow importing local modules
sys.path.append(str(Path(__file__).resolve().parent.parent))

from src.config import Config
from src.data.splitting import DataSplitter


def filter_by_activity(interactions_df: pd.DataFrame, min_user_interactions: int, min_item_interactions: int) -> pd.DataFrame:
    """
    Filters the interactions DataFrame to ensure users and items have a minimum number of interactions.

    This function first filters items, then users, to ensure that only active entities
    are included in the final dataset.

    Args:
        interactions_df: The input DataFrame containing 'user_id' and 'item_id' columns.
        min_user_interactions: Minimum number of interactions required for a user to be included.
        min_item_interactions: Minimum number of interactions required for an item to be included.

    Returns:
        A filtered DataFrame containing only interactions from active users and items.
    """
    # Filter by item interactions
    if min_item_interactions > 0:
        item_counts = interactions_df['item_id'].value_counts()
        active_items = item_counts[item_counts >= min_item_interactions].index
        interactions_df = interactions_df[interactions_df['item_id'].isin(active_items)]
        print(f"Filtered by item activity (min {min_item_interactions}): {len(interactions_df)} interactions, {interactions_df['item_id'].nunique()} items remain.")

    # Filter by user interactions
    if min_user_interactions > 0:
        user_counts = interactions_df['user_id'].value_counts()
        active_users = user_counts[user_counts >= min_user_interactions].index
        interactions_df = interactions_df[interactions_df['user_id'].isin(active_users)]
        print(f"Filtered by user activity (min {min_user_interactions}): {len(interactions_df)} interactions, {interactions_df['user_id'].nunique()} users remain.")
    return interactions_df


def create_splits(config_path, output_dir="data/splits"):
    """
    Creates standardized train/validation/test splits for fair evaluation of recommender systems.

    The split ratios, minimum interactions per user/item, and random state are
    read from the provided configuration file. The splitting strategy defaults
    to stratified.

    Args:
        config_path: Path to the YAML configuration file.
        output_dir: Directory where the generated splits and metadata will be saved.
    """
    
    # Load configuration from the specified YAML file.
    config = Config.from_yaml(config_path)
    # Access the data splitting configuration directly.
    splitting_config = config.data.splitting
    
    # Load processed interactions data. This data is expected to be
    # generated by a prior preprocessing step.
    interactions_df = pd.read_csv(config.data.processed_interactions_path)
    
    # Ensure the output directory for splits exists.
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    # Filter interactions based on minimum activity criteria as specified in the config.
    # This ensures that only users and items with sufficient historical interactions
    # are included in the dataset for splitting.
    interactions_df = filter_by_activity(
        interactions_df,
        min_user_interactions=splitting_config.min_interactions_per_user,
        min_item_interactions=splitting_config.min_interactions_per_item
    )

    if interactions_df.empty:
        print("Error: No interactions remaining after activity filtering. Cannot create splits.")
        sys.exit(1)
    
    # Initialize the data splitter with the configured random state for reproducibility.
    splitter = DataSplitter(random_state=splitting_config.random_state)
    
    # Calculate the total ratio for the training and validation sets.
    # This is used in the first step of a two-step stratified split.
    total_train_val_ratio = splitting_config.train_final_ratio + splitting_config.val_final_ratio
    
    # Perform basic validation for the configured ratios.
    if total_train_val_ratio <= 0:
        raise ValueError("Sum of train_final_ratio and val_final_ratio must be greater than zero.")
    if splitting_config.test_final_ratio < 0 or splitting_config.test_final_ratio >= 1:
        raise ValueError("test_final_ratio must be between 0 and 1 (exclusive of 1).")
    
    # Check if ratios sum to 1.0 and normalize them if there are floating-point inaccuracies.
    sum_of_all_ratios = splitting_config.train_final_ratio + splitting_config.val_final_ratio + splitting_config.test_final_ratio
    if abs(sum_of_all_ratios - 1.0) > 1e-6:
        print(f"Warning: Configured train_final_ratio ({splitting_config.train_final_ratio}), "
              f"val_final_ratio ({splitting_config.val_final_ratio}), and "
              f"test_final_ratio ({splitting_config.test_final_ratio}) do not sum to 1.0 (sum is {sum_of_all_ratios}). Normalizing...")
        if sum_of_all_ratios > 0:
            splitting_config.train_final_ratio /= sum_of_all_ratios
            splitting_config.val_final_ratio /= sum_of_all_ratios
            splitting_config.test_final_ratio /= sum_of_all_ratios
            total_train_val_ratio = splitting_config.train_final_ratio + splitting_config.val_final_ratio
        else: # Handle case where all ratios are zero, defaulting to a 60/20/20 split.
            print("All configured split ratios are zero. Defaulting to 60/20/20 split.")
            splitting_config.train_final_ratio = 0.6
            splitting_config.val_final_ratio = 0.2
            splitting_config.test_final_ratio = 0.2
            total_train_val_ratio = 0.8

    # First split: Separate out the test set from the remaining data (train_val_df).
    # A stratified split is used to ensure each user is represented in both parts,
    # as much as possible, for consistent evaluation.
    train_val_df, test_df = splitter.stratified_split(
        interactions_df, 
        train_ratio=total_train_val_ratio,
        min_interactions_per_user=splitting_config.min_interactions_per_user
    )
    
    # Second split: Divide the `train_val_df` into training and validation sets.
    # The ratio for this split is adjusted to reflect the final desired proportions
    # within the original dataset.
    train_ratio_for_second_split = (splitting_config.train_final_ratio / total_train_val_ratio) if total_train_val_ratio > 0 else 0.0
    
    train_df, val_df = splitter.stratified_split(
        train_val_df,
        train_ratio=train_ratio_for_second_split,
        min_interactions_per_user=splitting_config.min_interactions_per_user
    )
    
    # Save the generated data splits to CSV files.
    train_df.to_csv(output_path / "train.csv", index=False)
    val_df.to_csv(output_path / "val.csv", index=False)
    test_df.to_csv(output_path / "test.csv", index=False)
    
    # Create and save metadata about the generated splits.
    # This includes creation date, configuration used, dataset sizes,
    # and unique user/item counts in each split.
    metadata = {
        "creation_date": pd.Timestamp.now().isoformat(),
        "config_file": config_path,
        "total_interactions": len(interactions_df),
        "train_size": len(train_df),
        "val_size": len(val_df),
        "test_size": len(test_df),
        "train_users": train_df['user_id'].nunique(),
        "val_users": val_df['user_id'].nunique(),
        "test_users": test_df['user_id'].nunique(),
        "train_items": train_df['item_id'].nunique(),
        "val_items": val_df['item_id'].nunique(),
        "test_items": test_df['item_id'].nunique(),
        "data_hash": hashlib.md5(interactions_df.to_csv().encode()).hexdigest(),
        "split_ratios_configured": {
            "train_final_ratio": splitting_config.train_final_ratio,
            "val_final_ratio": splitting_config.val_final_ratio,
            "test_final_ratio": splitting_config.test_final_ratio
        },
        "random_state_used": splitting_config.random_state,
        "min_interactions_per_user_used": splitting_config.min_interactions_per_user,
        "min_interactions_per_item_used": splitting_config.min_interactions_per_item
    }
    
    with open(output_path / "split_metadata.json", 'w') as f:
        json.dump(metadata, f, indent=2)
    
    print(f"Created evaluation splits in {output_path}")
    print(json.dumps(metadata, indent=2))

    # Validate for data leakage if specified in the configuration.
    if splitting_config.validate_no_leakage:
        print("\nPerforming leakage validation...")
        # Check overlap between training and test sets.
        train_test_stats = splitter.get_split_statistics(train_df, test_df)
        print(f"Train-Test Overlap: Users - {train_test_stats['user_overlap']} ({train_test_stats['user_overlap_ratio']:.2%}), Items - {train_test_stats['item_overlap']} ({train_test_stats['item_overlap_ratio']:.2%})")

        # Check overlap between training and validation sets.
        train_val_stats = splitter.get_split_statistics(train_df, val_df)
        print(f"Train-Validation Overlap: Users - {train_val_stats['user_overlap']} ({train_val_stats['user_overlap_ratio']:.2%}), Items - {train_val_stats['item_overlap']} ({train_val_stats['item_overlap_ratio']:.2%})")
        
        # Check overlap between validation and test sets.
        val_test_stats = splitter.get_split_statistics(val_df, test_df)
        print(f"Validation-Test Overlap: Users - {val_test_stats['user_overlap']} ({val_test_stats['user_overlap_ratio']:.2%}), Items - {val_test_stats['item_overlap']} ({val_test_stats['item_overlap_ratio']:.2%})")

        # For stratified splits, user/item overlap is expected and generally desired
        # for warm-start evaluation. The check here primarily confirms no *unexpected*
        # leakage (e.g., specific interactions being duplicated).
        if train_test_stats['user_overlap_ratio'] < 1.0 or train_test_stats['item_overlap_ratio'] < 1.0:
            print("Warning: Some users/items in test set are not present in training set. This might indicate a cold-start scenario in the test set or an issue with stratification.")
        
        # Further checks can be added here if specific "no leakage" criteria are desired
        # beyond what stratified split intrinsically provides (e.g., strictly no user/item overlap
        # as in user-based or item-based splits, if the strategy were different).
        print("Leakage validation completed. For stratified splits, user/item overlap between sets is expected as interactions from the same user/item are distributed across splits.")

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="Create standardized splits for evaluation")
    parser.add_argument('--config', type=str, default='configs/default_config.yaml',
                        help='Path to configuration file (e.g., configs/default_config.yaml)')
    args = parser.parse_args()
    
    create_splits(args.config)